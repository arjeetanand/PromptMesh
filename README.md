# PromptForge

**PromptForge** is a **model-agnostic prompt evaluation and optimization framework** that lets you compare prompt versions across multiple LLMs, score outputs using **LLM-as-a-Judge**, and automatically generate improved prompts based on observed failures.

It is designed as a **Model Comparison Platform (MCP)** for serious prompt engineering, evaluation, and experimentation.

---

## ğŸš€ Why PromptForge?

When building GenAI systems:

* Small prompt changes can lead to large output differences
* Comparing prompts across models is manual and error-prone
* Evaluating â€œqualityâ€ is subjective without structure
* Improving prompts often relies on intuition, not data

**PromptForge solves this by turning prompt engineering into a measurable, repeatable system.**

---

## âœ¨ Key Features

* **Prompt Versioning (YAML-based)**

  * Maintain multiple prompt versions per task
  * Easy diffing and iteration

* **Multi-Model Execution**

  * Run the same prompt against:

    * OCI GenAI (Command-A, Meta, Gemini, Grok, etc.)
    * Ollama (local models like Llama 3, Qwen)
    * Cohere / OpenAI (extensible)

* **LLM-as-a-Judge Evaluation**

  * Uses a judge model to score outputs on:

    * Accuracy
    * Completeness
    * Instruction adherence
    * Hallucination risk

* **Deterministic Scoring**

  * Weighted scoring function
  * Hard rule checks + soft LLM judgment

* **Automated Prompt Optimization**

  * Detects failure types (hallucination, accuracy loss, etc.)
  * Generates improved prompts automatically
  * Validates improvements before acceptance

* **Model-Agnostic Architecture**

  * All models conform to a single interface
  * Easy to add new providers

---

## ğŸ§  Architecture Overview

```
prompts/        â†’ Prompt definitions & versions (YAML)
models/         â†’ LLM adapters (OCI, Ollama, Cohere, OpenAI)
core/           â†’ Execution & prompt rendering
evaluation/     â†’ Rules, judge, scoring logic
comparison/     â†’ Prompt & model comparison orchestration
optimization/   â†’ Failure analysis & prompt refinement
main.py         â†’ Entry point / experiment runner
```

---

## ğŸ“ Folder Structure

```
mcp/
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ registry.py
â”‚   â””â”€â”€ versions/
â”‚       â””â”€â”€ summarization/
â”‚           â”œâ”€â”€ v1.yaml
â”‚           â””â”€â”€ v2.yaml
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ registry.py
â”‚   â”œâ”€â”€ oci_chat_model.py
â”‚   â”œâ”€â”€ ollama_model.py
â”‚   â”œâ”€â”€ cohere_model.py
â”‚   â””â”€â”€ openai_model.py
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ executor.py
â”‚   â”œâ”€â”€ result.py
â”‚   â””â”€â”€ types.py
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ rules.py
â”‚   â”œâ”€â”€ judge.py
â”‚   â”œâ”€â”€ scorer.py
â”‚   â””â”€â”€ types.py
â”‚
â”œâ”€â”€ comparison/
â”‚   â”œâ”€â”€ runner.py
â”‚   â”œâ”€â”€ ranker.py
â”‚   â””â”€â”€ types.py
â”‚
â”œâ”€â”€ optimization/
â”‚   â”œâ”€â”€ failure_analysis.py
â”‚   â”œâ”€â”€ meta_prompt.py
â”‚   â”œâ”€â”€ optimizer.py
â”‚   â””â”€â”€ validator.py
â”‚
â”œâ”€â”€ main.py
â””â”€â”€ README.md
```

---

## ğŸ›  Supported Models

### Local (via Ollama)

* `llama3`
* `llama3:8b`
* `qwen2.5`
* `llava`

### Cloud

* **OCI GenAI**

  * Command-A (judge + generation)
  * Meta / Gemini / Grok (generic chat)
* **Cohere (Public API)**
* **OpenAI** (extensible)

---

## â–¶ï¸ Running PromptForge

### 1. Setup environment

```bash
python -m venv venv
venv\Scripts\activate   # Windows
pip install -r requirements.txt
```

Ensure:

* OCI config is set up locally
* Ollama is running for local models

---

### 2. Define prompts

Create YAML files in:

```
prompts/versions/<task>/<version>.yaml
```

Example:

```yaml
template: |
  Summarize the following text:

  {{text}}
```

---

### 3. Run experiments

```bash
python main.py
```

This will:

1. Load prompt versions
2. Render prompts with inputs
3. Run them across selected models
4. Evaluate outputs
5. Rank prompts
6. Suggest improved prompts if failures are detected

---

## ğŸ“Š Example Output

```
PROMPT VERSION: v1
MODEL: command-a
SCORE: 6.8
BREAKDOWN: {accuracy: 8, completeness: 7, adherence: 7, hallucination: 2}

PROMPT VERSION: v2
MODEL: command-a
SCORE: 7.5
BREAKDOWN: {accuracy: 9, completeness: 8, adherence: 8, hallucination: 1}
```

---

## ğŸ” Evaluation Logic

Final score is computed as:

```
0.4 Ã— Accuracy
+ 0.3 Ã— Completeness
+ 0.2 Ã— Adherence
âˆ’ 0.1 Ã— Hallucination
```

This makes hallucination **explicitly penalized**.

---

## ğŸ§ª Prompt Optimization Loop

1. Detect failure type
2. Generate revised prompt using optimizer LLM
3. Re-evaluate outputs
4. Accept improvement only if score improves

This enables **self-improving prompts**.

---

## ğŸ” Security

* OCI credentials are **never committed**
* `.gitignore` excludes:

  * `ociConfig/`
  * virtual environments
  * generated artifacts

---

## ğŸ¯ Use Cases

* Prompt benchmarking
* Model comparison
* Regression testing prompts
* Reducing hallucinations
* GenAI experimentation platforms
* Research & internal tooling

---

## ğŸ§­ Roadmap

* Parallel execution
* Cost estimation per run
* Judge calibration
* JSON-schema output validation
* Web UI / dashboard
* CI-based prompt regression testing

---

## ğŸ“„ License

MIT License (recommended for open experimentation).

---

## âœï¸ Author

Built by **Arjeet Anand**
Focused on GenAI systems, evaluation, and cloud-scale LLM engineering.
