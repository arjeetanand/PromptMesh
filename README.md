# PromptMesh

**PromptMesh** is a **model-agnostic prompt evaluation and prompt evolution framework** that systematically compares, scores, and **iteratively improves prompts** across multiple Large Language Models (LLMs).

It treats prompt engineering as a **systems problem**, combining versioning, evaluation, LLM-as-a-Judge scoring, and convergence-controlled prompt evolution into a single platform.

---

## ğŸš€ Why PromptMesh?

In real-world GenAI systems:

* Small prompt changes can cause large behavioral shifts
* Prompt quality varies drastically across models
* Manual prompt iteration does not scale
* Hallucination control requires objective measurement

**PromptMesh turns prompt engineering into a measurable, repeatable, and evolvable process.**

---

## âœ¨ Core Capabilities

### Phase 1 â€” Prompt Evaluation & Comparison

Phase 1 establishes a rigorous evaluation baseline.

* **Prompt Versioning (YAML-first)**

  * Versioned prompts per task
  * Easy experimentation and rollback

* **Multi-Model Execution**

  * Execute the same prompt across heterogeneous backends:

    * OCI GenAI (Commandâ€‘A, Meta, Gemini, Grok)
    * Ollama (local models like LlamaÂ 3, Qwen)
    * Cohere / OpenAI (pluggable)

* **LLM-as-a-Judge Evaluation**

  * Independent judge model scores outputs on:

    * Accuracy
    * Completeness
    * Instruction adherence
    * Hallucination risk

* **Deterministic Scoring**

  * Weighted scoring formula
  * Hard rule checks + soft semantic judgment

---

### Phase 2 â€” Prompt Evolution Engine

Phase 2 upgrades PromptMesh from comparison to **self-improving prompt evolution**.

Instead of a single optimization step, prompts are **mutated, evaluated, selected, and evolved** until convergence.

#### What Phase 2 Adds

* **Prompt Mutation (`optimization/mutator.py`)**

  * Generates multiple candidate prompts per iteration
  * Each mutation targets a specific failure mode

* **Candidate Selection (`optimization/selector.py`)**

  * Executes all candidates
  * Scores each using the evaluation pipeline
  * Selects the highestâ€‘performing prompt

* **Evolution Loop (`optimization/evolver.py`)**

  * Iteratively refines prompts over generations
  * Stops when improvement falls below a threshold
  * Enforces antiâ€‘regression constraints (e.g. hallucination control)

* **Traceable Evolution History**

  * Stores prompt text, scores, and breakdowns per generation
  * Enables full auditability of prompt changes

This transforms PromptMesh into a **closedâ€‘loop prompt evolution system**.

---

## ğŸ§  System Architecture

```
prompts/        â†’ Versioned prompt definitions (YAML)
models/         â†’ Model adapters & registries
core/           â†’ Execution, prompt rendering
comparison/     â†’ Prompt/model comparison orchestration
evaluation/     â†’ Rules, judge, scoring logic
optimization/   â†’ Prompt mutation, evolution & validation
main.py         â†’ Experiment runner
```

---

## ğŸ“ Project Structure

```
mcp/
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ registry.py
â”‚   â””â”€â”€ versions/
â”‚       â””â”€â”€ summarization/
â”‚           â”œâ”€â”€ v1.yaml
â”‚           â””â”€â”€ v2.yaml
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ registry.py
â”‚   â”œâ”€â”€ constants.py
â”‚   â”œâ”€â”€ oci_chat_model.py
â”‚   â”œâ”€â”€ ollama_model.py
â”‚   â”œâ”€â”€ cohere_model.py
â”‚   â””â”€â”€ openai_model.py
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ executor.py
â”‚   â”œâ”€â”€ result.py
â”‚   â””â”€â”€ types.py
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ rules.py
â”‚   â”œâ”€â”€ judge.py
â”‚   â”œâ”€â”€ scorer.py
â”‚   â””â”€â”€ types.py
â”‚
â”œâ”€â”€ comparison/
â”‚   â”œâ”€â”€ runner.py
â”‚   â”œâ”€â”€ ranker.py
â”‚   â””â”€â”€ types.py
â”‚
â”œâ”€â”€ optimization/
â”‚   â”œâ”€â”€ failure_analysis.py
â”‚   â”œâ”€â”€ mutator.py
â”‚   â”œâ”€â”€ selector.py
â”‚   â”œâ”€â”€ evolver.py
â”‚   â”œâ”€â”€ meta_prompt.py
â”‚   â”œâ”€â”€ optimizer.py
â”‚   â””â”€â”€ validator.py
â”‚
â”œâ”€â”€ main.py
â””â”€â”€ README.md
```

---

## ğŸ›  Supported Models

### Local (Ollama)

* `llama3:latest`
* `llama3:8b`
* `qwen2.5:latest`
* `llava:latest`

### Cloud

* **OCI GenAI**

  * Commandâ€‘A (generation + judge)
  * Meta / Gemini / Grok (generic chat)
* **Cohere API**
* **OpenAI API** (extensible)

---

## â–¶ï¸ Running PromptMesh

### 1. Environment Setup

```bash
python -m venv venv
venv\Scripts\activate   # Windows
pip install -r requirements.txt
```

Prerequisites:

* OCI credentials configured locally
* Ollama running for local inference

---

### 2. Define Prompts

Create prompt versions under:

```
prompts/versions/<task>/<version>.yaml
```

Example:

```yaml
template: |
  Summarize the following text:

  {{text}}
```

---

### 3. Run Experiments

```bash
python main.py
```

Execution flow:

1. Load prompt versions
2. Render prompts with inputs
3. Execute across models
4. Evaluate outputs
5. Rank prompts
6. Evolve prompts (PhaseÂ 2)
7. Validate improvements

---

## ğŸ“Š Scoring Model

Final score is computed as:

```
0.4 Ã— Accuracy
+ 0.3 Ã— Completeness
+ 0.2 Ã— Adherence
âˆ’ 0.1 Ã— Hallucination
```

Hallucination is explicitly penalized.

---

## ğŸ” Security

* No credentials are committed
* `.gitignore` excludes:

  * OCI config files
  * Virtual environments
  * Generated artifacts

---

## ğŸ¯ Use Cases

* Prompt benchmarking
* Model comparison
* Prompt regression testing
* Hallucination reduction
* GenAI experimentation platforms
* Internal evaluation tooling

---

## ğŸ§­ Roadmap

* Parallel execution engine
* Cost estimation per run
* Judge calibration & ensembles
* JSONâ€‘schema output validation
* Prompt evolution visualizer
* Web UI / dashboard
* CIâ€‘based prompt regression testing

---

## ğŸ“„ License

MIT License

---

## âœï¸ Author

Built by **Arjeet Anand**
Focused on GenAI systems, prompt evaluation, and cloudâ€‘scale LLM engineering.
